import { LLMModel } from "@/types";

export const modelList: LLMModel[] = [
  // ---- Llama Family ----
  {
    id: "llama-3.2-1b",
    name: "Llama 3.2 1B",
    family: "Llama",
    developer: "Meta",
    parameterCount: 1,
    architecture: "dense",
    contextLength: 128000,
    license: "Llama License",
    commercialUse: true,
    releaseDate: "2024-09-25",
    categories: ["general", "multilingual"],
    japaneseSupport: "fair",
    vramRequirements: { fp16: 2, q8: 1, q4: 0.6 },
    recommendedVram: {
      context8k: { q4: 1.5, q8: 2, fp16: 3 },
      context32k: { q4: 2, q8: 2.5, fp16: 4 },
    },
    benchmarks: { mmluPro: 0.34 },
    ratings: { overall: 2, coding: 1, reasoning: 2, creative: 2, multilingual: 2 },
    quickStart: { ollama: "ollama run llama3.2:1b" },
    notes: "超軽量モデル。エッジデバイスやテスト用途に",
  },
  {
    id: "llama-3.2-3b",
    name: "Llama 3.2 3B",
    family: "Llama",
    developer: "Meta",
    parameterCount: 3,
    architecture: "dense",
    contextLength: 128000,
    license: "Llama License",
    commercialUse: true,
    releaseDate: "2024-09-25",
    categories: ["general", "multilingual"],
    japaneseSupport: "fair",
    vramRequirements: { fp16: 6, q8: 3, q4: 1.8 },
    recommendedVram: {
      context8k: { q4: 3, q8: 4.5, fp16: 8 },
      context32k: { q4: 4, q8: 6, fp16: 10 },
    },
    benchmarks: { mmluPro: 0.42 },
    ratings: { overall: 3, coding: 2, reasoning: 2, creative: 2, multilingual: 3 },
    quickStart: { ollama: "ollama run llama3.2:3b" },
    notes: "軽量だが実用的。シンプルなタスクに",
  },
  {
    id: "llama-3.3-8b",
    name: "Llama 3.3 8B",
    family: "Llama",
    developer: "Meta",
    parameterCount: 8,
    architecture: "dense",
    contextLength: 128000,
    license: "Llama License",
    commercialUse: true,
    releaseDate: "2025-01-15",
    categories: ["general", "coding", "multilingual"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 16, q8: 8, q4: 4.5 },
    recommendedVram: {
      context8k: { q4: 6, q8: 10, fp16: 18 },
      context32k: { q4: 8, q8: 13, fp16: 22 },
    },
    benchmarks: { mmluPro: 0.55, humanEval: 0.62 },
    ratings: { overall: 4, coding: 3, reasoning: 3, creative: 3, multilingual: 4 },
    quickStart: { ollama: "ollama run llama3.3:8b" },
    notes: "バランスの良い中型モデル。コスパ重視の方に",
  },
  {
    id: "llama-3.3-70b",
    name: "Llama 3.3 70B",
    family: "Llama",
    developer: "Meta",
    parameterCount: 70,
    architecture: "dense",
    contextLength: 128000,
    license: "Llama License",
    commercialUse: true,
    releaseDate: "2024-12-06",
    categories: ["general", "coding", "reasoning", "multilingual"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 140, q8: 70, q4: 40, q3: 30 },
    recommendedVram: {
      context8k: { q4: 44, q8: 76, fp16: 148 },
      context32k: { q4: 55, q8: 90, fp16: 170 },
    },
    benchmarks: { mmluPro: 0.70, humanEval: 0.78, mathScore: 0.73 },
    ratings: { overall: 5, coding: 4, reasoning: 4, creative: 4, multilingual: 4 },
    quickStart: { ollama: "ollama run llama3.3:70b" },
    notes: "オープンソース最高峰クラス。GPT-4レベルの性能",
  },
  {
    id: "llama-4-scout",
    name: "Llama 4 Scout",
    family: "Llama",
    developer: "Meta",
    parameterCount: 109,
    activeParameters: 17,
    architecture: "moe",
    contextLength: 512000,
    license: "Llama License",
    commercialUse: true,
    releaseDate: "2025-04-05",
    categories: ["general", "coding", "reasoning", "multilingual"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 218, q8: 109, q4: 60, q3: 45 },
    recommendedVram: {
      context8k: { q4: 65, q8: 115, fp16: 225 },
      context32k: { q4: 78, q8: 130, fp16: 250 },
    },
    benchmarks: { mmluPro: 0.74, humanEval: 0.81 },
    ratings: { overall: 5, coding: 4, reasoning: 5, creative: 4, multilingual: 5 },
    quickStart: { ollama: "ollama run llama4-scout" },
    notes: "MoEアーキテクチャ。512Kコンテキスト対応。活性パラメータ17Bで効率的",
  },
  {
    id: "llama-4-maverick",
    name: "Llama 4 Maverick",
    family: "Llama",
    developer: "Meta",
    parameterCount: 400,
    activeParameters: 17,
    architecture: "moe",
    contextLength: 1048576,
    license: "Llama License",
    commercialUse: true,
    releaseDate: "2025-04-05",
    categories: ["general", "coding", "reasoning", "multilingual", "creative"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 800, q8: 400, q4: 220, q3: 165 },
    recommendedVram: {
      context8k: { q4: 230, q8: 410, fp16: 810 },
      context32k: { q4: 260, q8: 445, fp16: 850 },
    },
    benchmarks: { mmluPro: 0.78, humanEval: 0.85 },
    ratings: { overall: 5, coding: 5, reasoning: 5, creative: 5, multilingual: 5 },
    quickStart: {},
    notes: "Llama 4最大モデル。1Mコンテキスト。非常に大きなVRAMが必要",
  },
  // ---- Qwen 3 Family ----
  {
    id: "qwen3-0.6b",
    name: "Qwen3 0.6B",
    family: "Qwen",
    developer: "Alibaba",
    parameterCount: 0.6,
    architecture: "dense",
    contextLength: 32000,
    license: "Apache 2.0",
    commercialUse: true,
    releaseDate: "2025-04-29",
    categories: ["general"],
    japaneseSupport: "fair",
    vramRequirements: { fp16: 1.2, q8: 0.6, q4: 0.4 },
    recommendedVram: {
      context8k: { q4: 1, q8: 1.5, fp16: 2.5 },
      context32k: { q4: 1.5, q8: 2, fp16: 3.5 },
    },
    benchmarks: {},
    ratings: { overall: 1, coding: 1, reasoning: 1, creative: 1, multilingual: 2 },
    quickStart: { ollama: "ollama run qwen3:0.6b" },
    notes: "最小サイズ。組み込み・テスト用",
  },
  {
    id: "qwen3-4b",
    name: "Qwen3 4B",
    family: "Qwen",
    developer: "Alibaba",
    parameterCount: 4,
    architecture: "dense",
    contextLength: 32000,
    license: "Apache 2.0",
    commercialUse: true,
    releaseDate: "2025-04-29",
    categories: ["general", "multilingual", "coding"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 8, q8: 4, q4: 2.5 },
    recommendedVram: {
      context8k: { q4: 4, q8: 6, fp16: 10 },
      context32k: { q4: 5.5, q8: 8, fp16: 13 },
    },
    benchmarks: { mmluPro: 0.48, humanEval: 0.55 },
    ratings: { overall: 3, coding: 3, reasoning: 3, creative: 3, multilingual: 4 },
    quickStart: { ollama: "ollama run qwen3:4b" },
    notes: "小型ながら高性能。日本語対応も良好",
  },
  {
    id: "qwen3-8b",
    name: "Qwen3 8B",
    family: "Qwen",
    developer: "Alibaba",
    parameterCount: 8,
    architecture: "dense",
    contextLength: 32000,
    license: "Apache 2.0",
    commercialUse: true,
    releaseDate: "2025-04-29",
    categories: ["general", "coding", "multilingual", "reasoning"],
    japaneseSupport: "excellent",
    vramRequirements: { fp16: 16, q8: 8, q4: 4.5 },
    recommendedVram: {
      context8k: { q4: 6, q8: 10, fp16: 18 },
      context32k: { q4: 8, q8: 13, fp16: 22 },
    },
    benchmarks: { mmluPro: 0.57, humanEval: 0.68 },
    ratings: { overall: 4, coding: 4, reasoning: 3, creative: 3, multilingual: 5 },
    quickStart: { ollama: "ollama run qwen3:8b" },
    notes: "サイズ対性能比が優秀。日本語に強い",
  },
  {
    id: "qwen3-14b",
    name: "Qwen3 14B",
    family: "Qwen",
    developer: "Alibaba",
    parameterCount: 14,
    architecture: "dense",
    contextLength: 32000,
    license: "Apache 2.0",
    commercialUse: true,
    releaseDate: "2025-04-29",
    categories: ["general", "coding", "reasoning", "multilingual"],
    japaneseSupport: "excellent",
    vramRequirements: { fp16: 28, q8: 14, q4: 8 },
    recommendedVram: {
      context8k: { q4: 10, q8: 17, fp16: 31 },
      context32k: { q4: 14, q8: 22, fp16: 38 },
    },
    benchmarks: { mmluPro: 0.63, humanEval: 0.74 },
    ratings: { overall: 4, coding: 4, reasoning: 4, creative: 3, multilingual: 5 },
    quickStart: { ollama: "ollama run qwen3:14b" },
    notes: "14Bでトップクラスの性能。日本語◎",
  },
  {
    id: "qwen3-32b",
    name: "Qwen3 32B",
    family: "Qwen",
    developer: "Alibaba",
    parameterCount: 32,
    architecture: "dense",
    contextLength: 32000,
    license: "Qwen License",
    commercialUse: true,
    releaseDate: "2025-04-29",
    categories: ["general", "coding", "reasoning", "multilingual", "creative"],
    japaneseSupport: "excellent",
    vramRequirements: { fp16: 64, q8: 32, q4: 18, q3: 14 },
    recommendedVram: {
      context8k: { q4: 22, q8: 36, fp16: 69 },
      context32k: { q4: 28, q8: 45, fp16: 80 },
    },
    benchmarks: { mmluPro: 0.69, humanEval: 0.80, mathScore: 0.72 },
    ratings: { overall: 5, coding: 5, reasoning: 4, creative: 4, multilingual: 5 },
    quickStart: { ollama: "ollama run qwen3:32b" },
    notes: "Qwen3のスイートスポット。コーディングと日本語に特に強い",
  },
  {
    id: "qwen3-30b-a3b",
    name: "Qwen3 30B-A3B (MoE)",
    family: "Qwen",
    developer: "Alibaba",
    parameterCount: 30,
    activeParameters: 3,
    architecture: "moe",
    contextLength: 32000,
    license: "Apache 2.0",
    commercialUse: true,
    releaseDate: "2025-04-29",
    categories: ["general", "multilingual"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 60, q8: 30, q4: 17, q3: 13 },
    recommendedVram: {
      context8k: { q4: 20, q8: 34, fp16: 65 },
      context32k: { q4: 25, q8: 42, fp16: 75 },
    },
    benchmarks: { mmluPro: 0.52, humanEval: 0.60 },
    ratings: { overall: 3, coding: 3, reasoning: 3, creative: 3, multilingual: 4 },
    quickStart: { ollama: "ollama run qwen3:30b-a3b" },
    notes: "MoEモデル。総パラメータ30Bだが活性3Bで軽量推論",
  },
  {
    id: "qwen3-235b-a22b",
    name: "Qwen3 235B-A22B (MoE)",
    family: "Qwen",
    developer: "Alibaba",
    parameterCount: 235,
    activeParameters: 22,
    architecture: "moe",
    contextLength: 32000,
    license: "Qwen License",
    commercialUse: true,
    releaseDate: "2025-04-29",
    categories: ["general", "coding", "reasoning", "multilingual", "creative", "agent"],
    japaneseSupport: "excellent",
    vramRequirements: { fp16: 470, q8: 235, q4: 130, q3: 100 },
    recommendedVram: {
      context8k: { q4: 140, q8: 245, fp16: 480 },
      context32k: { q4: 165, q8: 275, fp16: 520 },
    },
    benchmarks: { mmluPro: 0.76, humanEval: 0.85, mathScore: 0.80 },
    ratings: { overall: 5, coding: 5, reasoning: 5, creative: 5, multilingual: 5 },
    quickStart: { ollama: "ollama run qwen3:235b-a22b" },
    notes: "Qwen3最大モデル。フロンティアクラスの性能",
  },
  // ---- DeepSeek Family ----
  {
    id: "deepseek-r1-1.5b",
    name: "DeepSeek R1 1.5B (蒸留)",
    family: "DeepSeek",
    developer: "DeepSeek",
    parameterCount: 1.5,
    architecture: "dense",
    contextLength: 64000,
    license: "MIT",
    commercialUse: true,
    releaseDate: "2025-01-20",
    categories: ["reasoning", "math"],
    japaneseSupport: "fair",
    vramRequirements: { fp16: 3, q8: 1.5, q4: 1 },
    recommendedVram: {
      context8k: { q4: 2, q8: 3, fp16: 5 },
      context32k: { q4: 3, q8: 4, fp16: 7 },
    },
    benchmarks: { mmluPro: 0.35, mathScore: 0.55 },
    ratings: { overall: 2, coding: 2, reasoning: 3, creative: 1, multilingual: 2 },
    quickStart: { ollama: "ollama run deepseek-r1:1.5b" },
    notes: "推論特化の蒸留モデル。軽量で数学に強い",
  },
  {
    id: "deepseek-r1-7b",
    name: "DeepSeek R1 7B (蒸留)",
    family: "DeepSeek",
    developer: "DeepSeek",
    parameterCount: 7,
    architecture: "dense",
    contextLength: 64000,
    license: "MIT",
    commercialUse: true,
    releaseDate: "2025-01-20",
    categories: ["reasoning", "math", "coding"],
    japaneseSupport: "fair",
    vramRequirements: { fp16: 14, q8: 7, q4: 4 },
    recommendedVram: {
      context8k: { q4: 6, q8: 9, fp16: 17 },
      context32k: { q4: 8, q8: 12, fp16: 21 },
    },
    benchmarks: { mmluPro: 0.50, humanEval: 0.60, mathScore: 0.68 },
    ratings: { overall: 3, coding: 3, reasoning: 4, creative: 2, multilingual: 3 },
    quickStart: { ollama: "ollama run deepseek-r1:7b" },
    notes: "推論特化。Chain-of-Thoughtに強い",
  },
  {
    id: "deepseek-r1-14b",
    name: "DeepSeek R1 14B (蒸留)",
    family: "DeepSeek",
    developer: "DeepSeek",
    parameterCount: 14,
    architecture: "dense",
    contextLength: 64000,
    license: "MIT",
    commercialUse: true,
    releaseDate: "2025-01-20",
    categories: ["reasoning", "math", "coding"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 28, q8: 14, q4: 8 },
    recommendedVram: {
      context8k: { q4: 10, q8: 17, fp16: 31 },
      context32k: { q4: 14, q8: 22, fp16: 38 },
    },
    benchmarks: { mmluPro: 0.60, humanEval: 0.70, mathScore: 0.75 },
    ratings: { overall: 4, coding: 4, reasoning: 5, creative: 3, multilingual: 3 },
    quickStart: { ollama: "ollama run deepseek-r1:14b" },
    notes: "14Bサイズで優秀な推論能力",
  },
  {
    id: "deepseek-r1-32b",
    name: "DeepSeek R1 32B (蒸留)",
    family: "DeepSeek",
    developer: "DeepSeek",
    parameterCount: 32,
    architecture: "dense",
    contextLength: 64000,
    license: "MIT",
    commercialUse: true,
    releaseDate: "2025-01-20",
    categories: ["reasoning", "math", "coding", "general"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 64, q8: 32, q4: 18, q3: 14 },
    recommendedVram: {
      context8k: { q4: 22, q8: 36, fp16: 69 },
      context32k: { q4: 28, q8: 45, fp16: 80 },
    },
    benchmarks: { mmluPro: 0.67, humanEval: 0.76, mathScore: 0.80 },
    ratings: { overall: 4, coding: 4, reasoning: 5, creative: 3, multilingual: 4 },
    quickStart: { ollama: "ollama run deepseek-r1:32b" },
    notes: "推論特化の32B。数学・コーディングに非常に強い",
  },
  {
    id: "deepseek-r1-70b",
    name: "DeepSeek R1 70B (蒸留)",
    family: "DeepSeek",
    developer: "DeepSeek",
    parameterCount: 70,
    architecture: "dense",
    contextLength: 64000,
    license: "MIT",
    commercialUse: true,
    releaseDate: "2025-01-20",
    categories: ["reasoning", "math", "coding", "general"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 140, q8: 70, q4: 40, q3: 30 },
    recommendedVram: {
      context8k: { q4: 44, q8: 76, fp16: 148 },
      context32k: { q4: 55, q8: 90, fp16: 170 },
    },
    benchmarks: { mmluPro: 0.72, humanEval: 0.82, mathScore: 0.85 },
    ratings: { overall: 5, coding: 5, reasoning: 5, creative: 4, multilingual: 4 },
    quickStart: { ollama: "ollama run deepseek-r1:70b" },
    notes: "推論性能トップクラス。GPT-4oに匹敵",
  },
  {
    id: "deepseek-v3-671b",
    name: "DeepSeek V3 671B (MoE)",
    family: "DeepSeek",
    developer: "DeepSeek",
    parameterCount: 671,
    activeParameters: 37,
    architecture: "moe",
    contextLength: 128000,
    license: "DeepSeek License",
    commercialUse: true,
    releaseDate: "2024-12-26",
    categories: ["general", "coding", "reasoning", "multilingual", "creative"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 1342, q8: 671, q4: 370, q3: 280 },
    recommendedVram: {
      context8k: { q4: 380, q8: 685, fp16: 1360 },
      context32k: { q4: 420, q8: 730, fp16: 1420 },
    },
    benchmarks: { mmluPro: 0.75, humanEval: 0.84 },
    ratings: { overall: 5, coding: 5, reasoning: 5, creative: 4, multilingual: 5 },
    quickStart: {},
    notes: "671B MoE。非常に大きなVRAMが必要。マルチGPU必須",
  },
  // ---- Gemma 3 Family ----
  {
    id: "gemma3-1b",
    name: "Gemma 3 1B",
    family: "Gemma",
    developer: "Google",
    parameterCount: 1,
    architecture: "dense",
    contextLength: 32000,
    license: "Gemma License",
    commercialUse: true,
    releaseDate: "2025-03-12",
    categories: ["general"],
    japaneseSupport: "fair",
    vramRequirements: { fp16: 2, q8: 1, q4: 0.6 },
    recommendedVram: {
      context8k: { q4: 1.5, q8: 2, fp16: 3 },
      context32k: { q4: 2, q8: 2.5, fp16: 4 },
    },
    benchmarks: {},
    ratings: { overall: 2, coding: 1, reasoning: 1, creative: 2, multilingual: 2 },
    quickStart: { ollama: "ollama run gemma3:1b" },
    notes: "超軽量Gemma。エッジ用途",
  },
  {
    id: "gemma3-4b",
    name: "Gemma 3 4B",
    family: "Gemma",
    developer: "Google",
    parameterCount: 4,
    architecture: "dense",
    contextLength: 128000,
    license: "Gemma License",
    commercialUse: true,
    releaseDate: "2025-03-12",
    categories: ["general", "multilingual"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 8, q8: 4, q4: 2.5 },
    recommendedVram: {
      context8k: { q4: 4, q8: 6, fp16: 10 },
      context32k: { q4: 5.5, q8: 8, fp16: 13 },
    },
    benchmarks: { mmluPro: 0.46 },
    ratings: { overall: 3, coding: 2, reasoning: 3, creative: 3, multilingual: 3 },
    quickStart: { ollama: "ollama run gemma3:4b" },
    notes: "128Kコンテキスト対応。バランスの良い小型モデル",
  },
  {
    id: "gemma3-12b",
    name: "Gemma 3 12B",
    family: "Gemma",
    developer: "Google",
    parameterCount: 12,
    architecture: "dense",
    contextLength: 128000,
    license: "Gemma License",
    commercialUse: true,
    releaseDate: "2025-03-12",
    categories: ["general", "coding", "multilingual"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 24, q8: 12, q4: 7 },
    recommendedVram: {
      context8k: { q4: 9, q8: 15, fp16: 27 },
      context32k: { q4: 12, q8: 19, fp16: 33 },
    },
    benchmarks: { mmluPro: 0.58, humanEval: 0.65 },
    ratings: { overall: 4, coding: 3, reasoning: 4, creative: 3, multilingual: 4 },
    quickStart: { ollama: "ollama run gemma3:12b" },
    notes: "Google品質の12B。マルチモーダル対応",
  },
  {
    id: "gemma3-27b",
    name: "Gemma 3 27B",
    family: "Gemma",
    developer: "Google",
    parameterCount: 27,
    architecture: "dense",
    contextLength: 128000,
    license: "Gemma License",
    commercialUse: true,
    releaseDate: "2025-03-12",
    categories: ["general", "coding", "reasoning", "multilingual"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 54, q8: 27, q4: 15, q3: 12 },
    recommendedVram: {
      context8k: { q4: 19, q8: 31, fp16: 59 },
      context32k: { q4: 24, q8: 39, fp16: 68 },
    },
    benchmarks: { mmluPro: 0.65, humanEval: 0.72 },
    ratings: { overall: 4, coding: 4, reasoning: 4, creative: 4, multilingual: 4 },
    quickStart: { ollama: "ollama run gemma3:27b" },
    notes: "Gemma最大。コスパ良好な高性能モデル",
  },
  // ---- Mistral Family ----
  {
    id: "mistral-7b",
    name: "Mistral 7B v0.3",
    family: "Mistral",
    developer: "Mistral AI",
    parameterCount: 7,
    architecture: "dense",
    contextLength: 32000,
    license: "Apache 2.0",
    commercialUse: true,
    releaseDate: "2024-05-22",
    categories: ["general", "coding"],
    japaneseSupport: "fair",
    vramRequirements: { fp16: 14, q8: 7, q4: 4 },
    recommendedVram: {
      context8k: { q4: 6, q8: 9, fp16: 17 },
      context32k: { q4: 8, q8: 12, fp16: 21 },
    },
    benchmarks: { mmluPro: 0.48, humanEval: 0.55 },
    ratings: { overall: 3, coding: 3, reasoning: 3, creative: 3, multilingual: 3 },
    quickStart: { ollama: "ollama run mistral:7b" },
    notes: "定番の7Bモデル。Apache 2.0で商用利用自由",
  },
  {
    id: "mistral-small-24b",
    name: "Mistral Small 3.1 24B",
    family: "Mistral",
    developer: "Mistral AI",
    parameterCount: 24,
    architecture: "dense",
    contextLength: 128000,
    license: "Apache 2.0",
    commercialUse: true,
    releaseDate: "2025-03-18",
    categories: ["general", "coding", "reasoning", "multilingual"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 48, q8: 24, q4: 14, q3: 10 },
    recommendedVram: {
      context8k: { q4: 17, q8: 28, fp16: 53 },
      context32k: { q4: 22, q8: 35, fp16: 62 },
    },
    benchmarks: { mmluPro: 0.66, humanEval: 0.73 },
    ratings: { overall: 4, coding: 4, reasoning: 4, creative: 4, multilingual: 4 },
    quickStart: { ollama: "ollama run mistral-small:24b" },
    notes: "128Kコンテキスト。Apache 2.0。非常にバランスが良い",
  },
  {
    id: "mistral-large-123b",
    name: "Mistral Large 3 123B",
    family: "Mistral",
    developer: "Mistral AI",
    parameterCount: 123,
    architecture: "dense",
    contextLength: 128000,
    license: "Mistral Research License",
    commercialUse: false,
    releaseDate: "2025-03-05",
    categories: ["general", "coding", "reasoning", "multilingual", "agent"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 246, q8: 123, q4: 68, q3: 52 },
    recommendedVram: {
      context8k: { q4: 75, q8: 130, fp16: 255 },
      context32k: { q4: 90, q8: 150, fp16: 280 },
    },
    benchmarks: { mmluPro: 0.73, humanEval: 0.82 },
    ratings: { overall: 5, coding: 5, reasoning: 5, creative: 4, multilingual: 5 },
    quickStart: { ollama: "ollama run mistral-large:123b" },
    notes: "Mistral最高峰。研究ライセンス（商用不可）",
  },
  // ---- Phi-4 Family ----
  {
    id: "phi-4-14b",
    name: "Phi-4 14B",
    family: "Phi",
    developer: "Microsoft",
    parameterCount: 14,
    architecture: "dense",
    contextLength: 16000,
    license: "MIT",
    commercialUse: true,
    releaseDate: "2024-12-12",
    categories: ["reasoning", "math", "coding"],
    japaneseSupport: "fair",
    vramRequirements: { fp16: 28, q8: 14, q4: 8 },
    recommendedVram: {
      context8k: { q4: 10, q8: 17, fp16: 31 },
      context32k: { q4: 14, q8: 22, fp16: 38 },
    },
    benchmarks: { mmluPro: 0.62, humanEval: 0.75, mathScore: 0.78 },
    ratings: { overall: 4, coding: 4, reasoning: 5, creative: 3, multilingual: 2 },
    quickStart: { ollama: "ollama run phi4:14b" },
    notes: "数学と推論に特化。MIT License。日本語は弱め",
  },
  // ---- NVIDIA Nemotron ----
  {
    id: "nemotron-8b",
    name: "Nemotron 8B",
    family: "Nemotron",
    developer: "NVIDIA",
    parameterCount: 8,
    architecture: "dense",
    contextLength: 8000,
    license: "NVIDIA License",
    commercialUse: true,
    releaseDate: "2025-01-10",
    categories: ["general", "coding"],
    japaneseSupport: "fair",
    vramRequirements: { fp16: 16, q8: 8, q4: 4.5 },
    recommendedVram: {
      context8k: { q4: 6, q8: 10, fp16: 18 },
      context32k: { q4: 8, q8: 13, fp16: 22 },
    },
    benchmarks: { mmluPro: 0.53, humanEval: 0.62 },
    ratings: { overall: 3, coding: 3, reasoning: 3, creative: 3, multilingual: 2 },
    quickStart: { ollama: "ollama run nemotron:8b" },
    notes: "NVIDIA製。TRT-LLMとの相性が良い",
  },
  {
    id: "nemotron-51b",
    name: "Nemotron 51B",
    family: "Nemotron",
    developer: "NVIDIA",
    parameterCount: 51,
    architecture: "dense",
    contextLength: 128000,
    license: "NVIDIA License",
    commercialUse: true,
    releaseDate: "2025-02-01",
    categories: ["general", "coding", "reasoning"],
    japaneseSupport: "fair",
    vramRequirements: { fp16: 102, q8: 51, q4: 28, q3: 22 },
    recommendedVram: {
      context8k: { q4: 33, q8: 57, fp16: 110 },
      context32k: { q4: 42, q8: 70, fp16: 130 },
    },
    benchmarks: { mmluPro: 0.68, humanEval: 0.76 },
    ratings: { overall: 4, coding: 4, reasoning: 4, creative: 3, multilingual: 3 },
    quickStart: { ollama: "ollama run nemotron:51b" },
    notes: "NVIDIA製大型モデル。DGX Sparkに最適化",
  },
  // ---- Command R+ ----
  {
    id: "command-r-35b",
    name: "Command R 35B",
    family: "Command",
    developer: "Cohere",
    parameterCount: 35,
    architecture: "dense",
    contextLength: 128000,
    license: "CC-BY-NC",
    commercialUse: false,
    releaseDate: "2024-03-11",
    categories: ["general", "rag", "multilingual"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 70, q8: 35, q4: 20, q3: 15 },
    recommendedVram: {
      context8k: { q4: 24, q8: 40, fp16: 76 },
      context32k: { q4: 30, q8: 50, fp16: 88 },
    },
    benchmarks: { mmluPro: 0.58 },
    ratings: { overall: 3, coding: 3, reasoning: 3, creative: 3, multilingual: 4 },
    quickStart: { ollama: "ollama run command-r:35b" },
    notes: "RAGに特化。128Kコンテキストで長文対応",
  },
  {
    id: "command-r-plus-104b",
    name: "Command R+ 104B",
    family: "Command",
    developer: "Cohere",
    parameterCount: 104,
    architecture: "dense",
    contextLength: 128000,
    license: "CC-BY-NC",
    commercialUse: false,
    releaseDate: "2024-04-04",
    categories: ["general", "rag", "multilingual", "agent"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 208, q8: 104, q4: 58, q3: 44 },
    recommendedVram: {
      context8k: { q4: 64, q8: 112, fp16: 218 },
      context32k: { q4: 78, q8: 130, fp16: 245 },
    },
    benchmarks: { mmluPro: 0.65, humanEval: 0.70 },
    ratings: { overall: 4, coding: 3, reasoning: 4, creative: 4, multilingual: 5 },
    quickStart: { ollama: "ollama run command-r-plus:104b" },
    notes: "RAG・エージェント向け大型モデル。多言語に強い",
  },
  // ---- GLM-4 ----
  {
    id: "glm-4-9b",
    name: "GLM-4 9B",
    family: "GLM",
    developer: "Zhipu AI",
    parameterCount: 9,
    architecture: "dense",
    contextLength: 128000,
    license: "GLM License",
    commercialUse: true,
    releaseDate: "2024-06-05",
    categories: ["general", "multilingual"],
    japaneseSupport: "good",
    vramRequirements: { fp16: 18, q8: 9, q4: 5 },
    recommendedVram: {
      context8k: { q4: 7, q8: 11, fp16: 20 },
      context32k: { q4: 9, q8: 14, fp16: 24 },
    },
    benchmarks: { mmluPro: 0.52 },
    ratings: { overall: 3, coding: 3, reasoning: 3, creative: 3, multilingual: 4 },
    quickStart: { ollama: "ollama run glm4:9b" },
    notes: "中国語・日本語に強い。128Kコンテキスト",
  },
];
